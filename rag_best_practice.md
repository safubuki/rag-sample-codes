# RAGå®Ÿè£…ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€RAGã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…æ™‚ã«å½¹ç«‹ã¤ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€è¨­è¨ˆåŸå‰‡ã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•ã€ç¶™ç¶šçš„æ”¹å–„ã®ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

## å®Ÿè£…æ™‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### RAGã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã®åŸå‰‡

#### ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥

> **ğŸ’¡ å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**  
> ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¯RAGã‚·ã‚¹ãƒ†ãƒ ã®æ ¹å¹¹ã¨ãªã‚‹æŠ€è¡“ã§ã€æ–‡æ›¸ã‚’ã©ã®ã‚ˆã†ã«åˆ†å‰²ã™ã‚‹ã‹ãŒæ¤œç´¢å“è³ªã«ç›´çµã—ã¾ã™ã€‚å˜ç´”ãªæ–‡å­—æ•°åˆ†å‰²ã§ã¯ãªãã€æ–‡æ›¸ã®æ§‹é€ ã¨å†…å®¹ã‚’ç†è§£ã—ãŸæˆ¦ç•¥çš„ãªåˆ†å‰²ãŒé‡è¦ã§ã™ã€‚

**é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®é¸æŠ**

> **ğŸ“š èƒŒæ™¯**  
> ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯ã€Œæƒ…å ±ã®å®Œçµæ€§ã€ã¨ã€Œæ¤œç´¢ã®ç²¾åº¦ã€ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æ±ºå®šã—ã¾ã™ã€‚å°ã•ã™ãã‚‹ã¨æ–‡è„ˆãŒå¤±ã‚ã‚Œã€å¤§ãã™ãã‚‹ã¨ç„¡é–¢ä¿‚ãªæƒ…å ±ãŒæ··å…¥ã—ã¦ã—ã¾ã„ã¾ã™ã€‚æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã”ã¨ã«æœ€é©åŒ–ã™ã‚‹ã“ã¨ã§ã€å„ç”¨é€”ã«å¿œã˜ãŸæœ€é«˜ã®æ€§èƒ½ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

```python
# æ–‡æ›¸ã®æ€§è³ªã«å¿œã˜ãŸãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®èª¿æ•´
class ChunkingConfig:
    # æŠ€è¡“æ–‡æ›¸ãƒ»ãƒãƒ‹ãƒ¥ã‚¢ãƒ«: è©³ç´°ãªæ‰‹é †ã‚’å«ã‚€ãŸã‚å¤§ãã‚
    TECHNICAL_DOC = {"chunk_size": 800, "chunk_overlap": 150}
    
    # FAQãƒ»Q&A: ä¸€å•ä¸€ç­”å½¢å¼ã®ãŸã‚ä¸­ç¨‹åº¦
    FAQ_DOC = {"chunk_size": 400, "chunk_overlap": 100}
    
    # ãƒãƒ£ãƒƒãƒˆãƒ»å¯¾è©±ãƒ­ã‚°: çŸ­ã„ã‚„ã‚Šå–ã‚Šã®ãŸã‚å°ã•ã‚
    CHAT_LOG = {"chunk_size": 200, "chunk_overlap": 50}

def get_chunking_config(doc_type: str) -> dict:
    """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©ãªãƒãƒ£ãƒ³ã‚¯è¨­å®šã‚’è¿”ã™"""
    configs = {
        "technical": ChunkingConfig.TECHNICAL_DOC,
        "faq": ChunkingConfig.FAQ_DOC,
        "chat": ChunkingConfig.CHAT_LOG
    }
    return configs.get(doc_type, ChunkingConfig.TECHNICAL_DOC)
```

**æ„å‘³çš„ãªåˆ†å‰²ãƒã‚¤ãƒ³ãƒˆã®æ´»ç”¨**

> **ğŸ¯ ãªãœé‡è¦ã‹**  
> æ©Ÿæ¢°çš„ãªæ–‡å­—æ•°åˆ†å‰²ã§ã¯ãªãã€æ–‡æ›¸ã®è«–ç†æ§‹é€ ï¼ˆè¦‹å‡ºã—ã€æ®µè½ã€æ–‡ï¼‰ã«æ²¿ã£ã¦åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€æ„å‘³çš„ãªä¸€è²«æ€§ã‚’ä¿ã¡ãªãŒã‚‰æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç‰¹ã«Markdownå½¢å¼ã®æŠ€è¡“æ–‡æ›¸ã§ã¯ã€éšå±¤æ§‹é€ ã‚’æ´»ç”¨ã—ãŸåˆ†å‰²ãŒåŠ¹æœçš„ã§ã™ã€‚

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def create_smart_splitter(chunk_size: int = 800, chunk_overlap: int = 150):
    """æ„å‘³çš„ãªå¢ƒç•Œã‚’è€ƒæ…®ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ"""
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        # åˆ†å‰²ã®å„ªå…ˆé †ä½: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’ æ®µè½ â†’ æ–‡ â†’ å˜èª â†’ æ–‡å­—
        separators=[
            "\n## ",      # Markdownã‚»ã‚¯ã‚·ãƒ§ãƒ³
            "\n### ",     # Markdownã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            "\n\n",       # æ®µè½
            "\n",         # æ”¹è¡Œ
            "ã€‚",         # æ—¥æœ¬èªã®æ–‡æœ«
            ".",          # è‹±èªã®æ–‡æœ«
            "ï¼",         # æ„Ÿå˜†ç¬¦
            "!",
            "ï¼Ÿ",         # ç–‘å•ç¬¦
            "?",
            " ",          # ã‚¹ãƒšãƒ¼ã‚¹
            ""            # æ–‡å­—ãƒ¬ãƒ™ãƒ«
        ],
        length_function=len,
        is_separator_regex=False
    )
```

#### æ¤œç´¢ç²¾åº¦ã®å‘ä¸Š

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè£…**

> **ğŸš€ å®Ÿè·µã®ãƒ¡ãƒªãƒƒãƒˆ**  
> å˜ä¸€ã®æ¤œç´¢æ‰‹æ³•ã«ä¾å­˜ã›ãšã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãã‚Œãã‚Œã®å¼±ç‚¹ã‚’è£œå®Œã—åˆã„ã¾ã™ã€‚ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯æ„å‘³çš„é¡ä¼¼æ€§ã«å¼·ãã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯å›ºæœ‰åè©ã‚„å°‚é–€ç”¨èªã«å¼·ã„ã¨ã„ã†ç‰¹æ€§ã‚’æ´»ç”¨ã§ãã¾ã™ã€‚

```python
from typing import List, Dict
import numpy as np

def hybrid_search(
    query: str, 
    vector_store, 
    keyword_weights: Dict[str, float] = None,
    vector_weight: float = 0.7,
    keyword_weight: float = 0.3,
    k: int = 5
) -> List:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸæ¤œç´¢"""
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    vector_results = vector_store.similarity_search_with_score(query, k=k*2)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
    keyword_results = keyword_search(query, vector_store, k=k*2)
    
    # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ã¨çµ±åˆ
    combined_results = {}
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã®å‡¦ç†
    for doc, score in vector_results:
        doc_id = doc.metadata.get('id', hash(doc.page_content))
        combined_results[doc_id] = {
            'doc': doc,
            'vector_score': 1 - score,  # è·é›¢ã‚’é¡ä¼¼åº¦ã«å¤‰æ›
            'keyword_score': 0
        }
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã®å‡¦ç†
    for doc, score in keyword_results:
        doc_id = doc.metadata.get('id', hash(doc.page_content))
        if doc_id in combined_results:
            combined_results[doc_id]['keyword_score'] = score
        else:
            combined_results[doc_id] = {
                'doc': doc,
                'vector_score': 0,
                'keyword_score': score
            }
    
    # çµ±åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    for doc_id, data in combined_results.items():
        data['combined_score'] = (
            vector_weight * data['vector_score'] + 
            keyword_weight * data['keyword_score']
        )
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_results = sorted(
        combined_results.values(),
        key=lambda x: x['combined_score'],
        reverse=True
    )
    
    return [result['doc'] for result in sorted_results[:k]]
```

**ã‚¯ã‚¨ãƒªæ‹¡å¼µã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**

> **ğŸ” æŠ€è¡“çš„ãªä¾¡å€¤**  
> ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è§£æãƒ»æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€æ¤œç´¢ã®ç¶²ç¾…æ€§ã¨ç²¾åº¦ã‚’åŒæ™‚ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æ—¥æœ¬èªã®åŒç¾©èªå±•é–‹ã€æ™‚é–“çš„åˆ¶ç´„ã®è‡ªå‹•æ¤œå‡ºã€ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãªã©ã«ã‚ˆã‚Šã€ã‚ˆã‚Šé©åˆ‡ãªæ–‡æ›¸ã‚’ç‰¹å®šã§ãã¾ã™ã€‚

```python
import re
from datetime import datetime, timedelta

def enhance_query(query: str, context: Dict = None) -> Dict:
    """ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µãƒ»å¼·åŒ–ã™ã‚‹"""
    
    enhanced = {
        'original_query': query,
        'expanded_queries': [],
        'filters': {},
        'query_type': 'general'
    }
    
    # æ™‚é–“çš„ãªã‚¯ã‚¨ãƒªã®æ¤œå‡º
    if re.search(r'æœ€æ–°|ä»Šæ—¥|æ˜¨æ—¥|ä»Šé€±|ä»Šæœˆ', query):
        enhanced['query_type'] = 'temporal'
        enhanced['filters']['date_range'] = {
            'start': datetime.now() - timedelta(days=30),
            'end': datetime.now()
        }
    
    # ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã®æ¤œå‡º
    category_keywords = {
        'maintenance': ['ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹', 'ä¿å®ˆ', 'ç‚¹æ¤œ', 'æ•´å‚™'],
        'trouble': ['ãƒˆãƒ©ãƒ–ãƒ«', 'å•é¡Œ', 'ä¸å…·åˆ', 'ã‚¨ãƒ©ãƒ¼'],
        'procedure': ['æ‰‹é †', 'æ–¹æ³•', 'ã‚„ã‚Šæ–¹', 'æ“ä½œ']
    }
    
    for category, keywords in category_keywords.items():
        if any(keyword in query for keyword in keywords):
            enhanced['filters']['category'] = category
            enhanced['query_type'] = category
            break
    
    # åŒç¾©èªå±•é–‹
    synonyms = {
        'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹': ['ä¿å®ˆ', 'ç‚¹æ¤œ', 'æ•´å‚™', 'maintenance'],
        'ãƒˆãƒ©ãƒ–ãƒ«': ['å•é¡Œ', 'ä¸å…·åˆ', 'ã‚¨ãƒ©ãƒ¼', 'trouble', 'issue'],
        'æ‰‹é †': ['æ–¹æ³•', 'ã‚„ã‚Šæ–¹', 'æ“ä½œ', 'procedure', 'steps']
    }
    
    for word, syns in synonyms.items():
        if word in query:
            enhanced['expanded_queries'].extend([
                query.replace(word, syn) for syn in syns
            ])
    
    return enhanced
```

#### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

**æ®µéšçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰**

> **ğŸ­ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®åŸå‰‡**  
> åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å˜ãªã‚‹æŒ‡ç¤ºæ–‡ã§ã¯ãªãã€LLMãŒæœ€é©ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ã€Œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€ã§ã™ã€‚å½¹å‰²å®šç¾©ã€æ–‡è„ˆæä¾›ã€å‡ºåŠ›å½¢å¼ã®æŒ‡å®šã‚’æ§‹é€ åŒ–ã™ã‚‹ã“ã¨ã§ã€ä¸€è²«æ€§ã®ã‚ã‚‹é«˜å“è³ªãªå›ç­”ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
class PromptBuilder:
    """æ®µéšçš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.system_prompt = ""
        self.context_prompt = ""
        self.query_prompt = ""
        self.formatting_prompt = ""
    
    def set_system_role(self, domain: str = "general"):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ãƒ¼ãƒ«ã‚’è¨­å®š"""
        roles = {
            "technical": "ã‚ãªãŸã¯æŠ€è¡“ã‚µãƒãƒ¼ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚",
            "maintenance": "ã‚ãªãŸã¯è¨­å‚™ä¿å®ˆã®å°‚é–€å®¶ã§ã™ã€‚",
            "general": "ã‚ãªãŸã¯çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        }
        self.system_prompt = roles.get(domain, roles["general"])
        return self
    
    def add_context(self, documents: List, max_context_length: int = 2000):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¿½åŠ """
        context_parts = []
        current_length = 0
        
        for doc in documents:
            content = doc.page_content
            if current_length + len(content) > max_context_length:
                break
            context_parts.append(content)
            current_length += len(content)
        
        if context_parts:
            self.context_prompt = (
                "\nä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š\n\n" +
                "\n---\n".join(context_parts)
            )
        return self
    
    def set_query(self, query: str, query_type: str = "general"):
        """ã‚¯ã‚¨ãƒªã¨å›ç­”å½¢å¼ã‚’è¨­å®š"""
        formatting_rules = {
            "technical": "\n\nå…·ä½“çš„ãªæ‰‹é †ãŒã‚ã‚‹å ´åˆã¯ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ã€‚",
            "maintenance": "\n\nå®‰å…¨ä¸Šã®æ³¨æ„ç‚¹ãŒã‚ã‚‹å ´åˆã¯ã€å¿…ãšæœ€åˆã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚",
            "general": "\n\nç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚"
        }
        
        self.query_prompt = f"\n\nè³ªå•: {query}"
        self.formatting_prompt = formatting_rules.get(query_type, formatting_rules["general"])
        return self
    
    def build(self) -> str:
        """æœ€çµ‚çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        return (
            self.system_prompt +
            self.context_prompt +
            self.query_prompt +
            self.formatting_prompt
        ).strip()

# ä½¿ç”¨ä¾‹
def create_optimized_prompt(query: str, documents: List, query_type: str = "general") -> str:
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    return (
        PromptBuilder()
        .set_system_role(query_type)
        .add_context(documents)
        .set_query(query, query_type)
        .build()
    )
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°æˆ¦ç•¥

#### å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
import logging
from typing import Optional, Union
from functools import wraps
import time
import traceback

class RAGError(Exception):
    """RAGã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰ã®ã‚¨ãƒ©ãƒ¼"""
    pass

class RetryableError(RAGError):
    """å†è©¦è¡Œå¯èƒ½ãªã‚¨ãƒ©ãƒ¼"""
    pass

def retry_on_failure(max_retries: int = 3, delay: float = 1.0):
    """å¤±æ•—æ™‚ã«è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except RetryableError as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                        logging.warning(
                            f"Attempt {attempt + 1} failed: {e}. "
                            f"Retrying in {wait_time:.1f}s..."
                        )
                        time.sleep(wait_time)
                    else:
                        logging.error(f"All {max_retries + 1} attempts failed")
                except Exception as e:
                    # å†è©¦è¡Œä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«ç™ºç”Ÿ
                    logging.error(f"Non-retryable error: {e}")
                    raise
            
            raise last_exception
        return wrapper
    return decorator

@retry_on_failure(max_retries=3)
def generate_response_with_retry(prompt: str, model_client) -> str:
    """LLMå¿œç­”ç”Ÿæˆï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    try:
        response = model_client.generate_response(prompt)
        if not response or len(response.strip()) < 10:
            raise RetryableError("Empty or too short response")
        return response
    except ConnectionError as e:
        raise RetryableError(f"Connection failed: {e}")
    except ValueError as e:
        raise RAGError(f"Invalid input: {e}")  # å†è©¦è¡Œä¸å¯
```

#### æ§‹é€ åŒ–ãƒ­ã‚°ã®å®Ÿè£…

```python
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional

class StructuredLogger:
    """æ§‹é€ åŒ–ãƒ­ã‚°ç”¨ã®ãƒ­ã‚¬ãƒ¼"""
    
    def __init__(self, name: str, log_file: Optional[str] = None):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã®è¨­å®š
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆï¼‰
        if log_file:
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def log_query(self, query: str, query_type: str, user_id: str = None):
        """ã‚¯ã‚¨ãƒªãƒ­ã‚°"""
        log_data = {
            "event": "query_received",
            "timestamp": datetime.now().isoformat(),
            "query_length": len(query),
            "query_type": query_type,
            "user_id": user_id or "anonymous"
        }
        self.logger.info(f"QUERY: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_retrieval(self, retrieved_docs: List, query: str, retrieval_time: float):
        """æ¤œç´¢çµæœãƒ­ã‚°"""
        log_data = {
            "event": "documents_retrieved",
            "timestamp": datetime.now().isoformat(),
            "doc_count": len(retrieved_docs),
            "retrieval_time_ms": round(retrieval_time * 1000, 2),
            "doc_lengths": [len(doc.page_content) for doc in retrieved_docs[:3]]  # ä¸Šä½3ä»¶ã®ã¿
        }
        self.logger.info(f"RETRIEVAL: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_generation(self, prompt_length: int, response_length: int, generation_time: float):
        """ç”Ÿæˆçµæœãƒ­ã‚°"""
        log_data = {
            "event": "response_generated",
            "timestamp": datetime.now().isoformat(),
            "prompt_length": prompt_length,
            "response_length": response_length,
            "generation_time_ms": round(generation_time * 1000, 2),
            "tokens_per_second": round(response_length / max(generation_time, 0.001), 2)
        }
        self.logger.info(f"GENERATION: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_error(self, error: Exception, context: Dict[str, Any] = None):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"""
        log_data = {
            "event": "error_occurred",
            "timestamp": datetime.now().isoformat(),
            "error_type": type(error).__name__,
            "error_message": str(error),
            "context": context or {}
        }
        self.logger.error(f"ERROR: {json.dumps(log_data, ensure_ascii=False)}")
        self.logger.debug(f"Traceback: {traceback.format_exc()}")
```

### ç’°å¢ƒè¨­å®šã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

#### ç’°å¢ƒä¾å­˜è¨­å®šã®ç®¡ç†

```python
import os
from typing import Optional
from dataclasses import dataclass
from pathlib import Path

@dataclass
class Config:
    """è¨­å®šã‚¯ãƒ©ã‚¹"""
    # Google Cloudè¨­å®š
    google_cloud_project: str
    google_cloud_region: str = "us-central1"
    google_application_credentials: Optional[str] = None
    
    # RAGè¨­å®š
    chunk_size: int = 800
    chunk_overlap: int = 150
    max_retrieved_docs: int = 5
    
    # LLMè¨­å®š
    model_name: str = "gemini-1.5-pro"
    max_tokens: int = 1000
    temperature: float = 0.1
    
    # ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
    debug_mode: bool = False
    log_level: str = "INFO"
    max_query_length: int = 1000
    
    @classmethod
    def from_env(cls) -> "Config":
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        return cls(
            google_cloud_project=os.getenv("GOOGLE_CLOUD_PROJECT", ""),
            google_cloud_region=os.getenv("GOOGLE_CLOUD_REGION", "us-central1"),
            google_application_credentials=os.getenv("GOOGLE_APPLICATION_CREDENTIALS"),
            
            chunk_size=int(os.getenv("CHUNK_SIZE", "800")),
            chunk_overlap=int(os.getenv("CHUNK_OVERLAP", "150")),
            max_retrieved_docs=int(os.getenv("MAX_RETRIEVED_DOCS", "5")),
            
            model_name=os.getenv("MODEL_NAME", "gemini-1.5-pro"),
            max_tokens=int(os.getenv("MAX_TOKENS", "1000")),
            temperature=float(os.getenv("TEMPERATURE", "0.1")),
            
            debug_mode=os.getenv("DEBUG_MODE", "false").lower() == "true",
            log_level=os.getenv("LOG_LEVEL", "INFO"),
            max_query_length=int(os.getenv("MAX_QUERY_LENGTH", "1000"))
        )
    
    def validate(self) -> List[str]:
        """è¨­å®šã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        errors = []
        
        if not self.google_cloud_project:
            errors.append("GOOGLE_CLOUD_PROJECT is required")
        
        if self.google_application_credentials:
            if not Path(self.google_application_credentials).exists():
                errors.append(f"Credentials file not found: {self.google_application_credentials}")
        
        if self.chunk_size <= 0:
            errors.append("chunk_size must be positive")
        
        if self.chunk_overlap >= self.chunk_size:
            errors.append("chunk_overlap must be less than chunk_size")
        
        if self.max_retrieved_docs <= 0:
            errors.append("max_retrieved_docs must be positive")
        
        return errors
```

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–

```python
import hashlib
import secrets
from typing import Dict, List
import re

class SecurityManager:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.sensitive_patterns = [
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
            r'\b\d{3}-\d{4}-\d{4}\b',  # é›»è©±ç•ªå·
            r'\b\d{4}\s?\d{4}\s?\d{4}\s?\d{4}\b',  # ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ç•ªå·
            r'password\s*[:=]\s*\S+',  # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            r'api[_-]?key\s*[:=]\s*\S+',  # APIã‚­ãƒ¼
        ]
    
    def sanitize_query(self, query: str) -> str:
        """ã‚¯ã‚¨ãƒªã‹ã‚‰æ©Ÿå¯†æƒ…å ±ã‚’é™¤å»"""
        sanitized = query
        
        for pattern in self.sensitive_patterns:
            sanitized = re.sub(pattern, '[REDACTED]', sanitized, flags=re.IGNORECASE)
        
        return sanitized
    
    def hash_user_id(self, user_id: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’ãƒãƒƒã‚·ãƒ¥åŒ–"""
        return hashlib.sha256(user_id.encode()).hexdigest()[:16]
    
    def validate_input_length(self, text: str, max_length: int = 1000) -> bool:
        """å…¥åŠ›é•·ã®æ¤œè¨¼"""
        return len(text) <= max_length
    
    def check_malicious_patterns(self, text: str) -> List[str]:
        """æ‚ªæ„ã®ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯"""
        warnings = []
        
        # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        sql_patterns = [
            r'(union|select|insert|update|delete|drop)\s+',
            r'--',
            r'/\*.*\*/',
        ]
        
        for pattern in sql_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                warnings.append(f"Potential SQL injection pattern detected")
                break
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        script_patterns = [
            r'<script',
            r'javascript:',
            r'eval\s*\(',
        ]
        
        for pattern in script_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                warnings.append(f"Potential script injection pattern detected")
                break
        
        return warnings

def secure_query_handler(query: str, user_id: str = None) -> Dict:
    """ã‚»ã‚­ãƒ¥ã‚¢ãªã‚¯ã‚¨ãƒªå‡¦ç†"""
    security = SecurityManager()
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
    warnings = security.check_malicious_patterns(query)
    if warnings:
        return {
            "error": "Security validation failed",
            "warnings": warnings
        }
    
    # å…¥åŠ›é•·ãƒã‚§ãƒƒã‚¯
    if not security.validate_input_length(query):
        return {
            "error": "Query too long",
            "max_length": 1000
        }
    
    # æ©Ÿå¯†æƒ…å ±ã®é™¤å»
    sanitized_query = security.sanitize_query(query)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒãƒƒã‚·ãƒ¥åŒ–
    hashed_user_id = security.hash_user_id(user_id) if user_id else None
    
    return {
        "sanitized_query": sanitized_query,
        "user_hash": hashed_user_id,
        "original_length": len(query),
        "sanitized_length": len(sanitized_query)
    }
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

```python
import functools
import hashlib
import json
import time
from typing import Any, Dict, Optional
import redis

class CacheManager:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, redis_client: Optional[redis.Redis] = None):
        self.redis_client = redis_client
        self.memory_cache = {}
        self.cache_stats = {"hits": 0, "misses": 0}
    
    def _get_cache_key(self, query: str, context_hash: str = None) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        key_data = {
            "query": query.lower().strip(),
            "context": context_hash or ""
        }
        key_string = json.dumps(key_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€¤ã‚’å–å¾—"""
        # Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç¢ºèª
        if self.redis_client:
            try:
                cached = self.redis_client.get(key)
                if cached:
                    self.cache_stats["hits"] += 1
                    return json.loads(cached)
            except:
                pass
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç¢ºèª
        if key in self.memory_cache:
            cache_entry = self.memory_cache[key]
            if time.time() - cache_entry["timestamp"] < 3600:  # 1æ™‚é–“æœ‰åŠ¹
                self.cache_stats["hits"] += 1
                return cache_entry["data"]
            else:
                del self.memory_cache[key]
        
        self.cache_stats["misses"] += 1
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å€¤ã‚’è¨­å®š"""
        # Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if self.redis_client:
            try:
                self.redis_client.setex(
                    key, 
                    ttl, 
                    json.dumps(value, ensure_ascii=False)
                )
            except:
                pass
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.memory_cache[key] = {
            "data": value,
            "timestamp": time.time()
        }
    
    def get_stats(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        total = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = self.cache_stats["hits"] / total if total > 0 else 0
        
        return {
            "hits": self.cache_stats["hits"],
            "misses": self.cache_stats["misses"],
            "hit_rate": round(hit_rate, 3),
            "memory_cache_size": len(self.memory_cache)
        }

def cached_response(cache_manager: CacheManager, ttl: int = 3600):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = cache_manager._get_cache_key(
                str(args) + str(kwargs)
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
            cached_result = cache_manager.get(cache_key)
            if cached_result:
                return cached_result
            
            # é–¢æ•°ã‚’å®Ÿè¡Œ
            result = func(*args, **kwargs)
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            cache_manager.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

#### ãƒãƒƒãƒå‡¦ç†ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Callable, Any

class BatchProcessor:
    """ãƒãƒƒãƒå‡¦ç†ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
    
    def process_documents_parallel(
        self, 
        documents: List[str], 
        process_func: Callable,
        batch_size: int = 10
    ) -> List[Any]:
        """æ–‡æ›¸ã‚’ä¸¦åˆ—å‡¦ç†"""
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ãƒãƒƒãƒã«åˆ†å‰²
            batches = [
                documents[i:i + batch_size] 
                for i in range(0, len(documents), batch_size)
            ]
            
            # å„ãƒãƒƒãƒã‚’ä¸¦åˆ—å®Ÿè¡Œ
            future_to_batch = {
                executor.submit(process_func, batch): batch 
                for batch in batches
            }
            
            for future in as_completed(future_to_batch):
                try:
                    batch_result = future.result()
                    results.extend(batch_result)
                except Exception as e:
                    logging.error(f"Batch processing failed: {e}")
        
        return results
    
    async def async_pipeline(
        self, 
        data: Any, 
        processors: List[Callable]
    ) -> Any:
        """éåŒæœŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†"""
        result = data
        
        for processor in processors:
            if asyncio.iscoroutinefunction(processor):
                result = await processor(result)
            else:
                result = processor(result)
        
        return result

# ä½¿ç”¨ä¾‹
async def optimized_rag_pipeline(query: str, documents: List[str]) -> str:
    """æœ€é©åŒ–ã•ã‚ŒãŸRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    batch_processor = BatchProcessor()
    
    # ä¸¦åˆ—å‡¦ç†ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
    async def create_embeddings(docs):
        return await asyncio.to_thread(
            batch_processor.process_documents_parallel,
            docs,
            lambda batch: [embed_document(doc) for doc in batch]
        )
    
    # éåŒæœŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    processors = [
        lambda x: preprocess_documents(x),
        create_embeddings,
        lambda x: search_similar_documents(query, x),
        lambda x: generate_response(query, x)
    ]
    
    return await batch_processor.async_pipeline(documents, processors)
```

### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨æœ¬ç•ªé‹ç”¨

#### æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥

```python
from enum import Enum
from typing import Dict, List
import random

class DeploymentStage(Enum):
    DEVELOPMENT = "dev"
    STAGING = "staging"
    CANARY = "canary"
    PRODUCTION = "prod"

class FeatureFlags:
    """æ©Ÿèƒ½ãƒ•ãƒ©ã‚°ç®¡ç†"""
    
    def __init__(self):
        self.flags = {
            "hybrid_search": {
                "dev": True,
                "staging": True,
                "canary": 0.1,  # 10%ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
                "prod": False
            },
            "advanced_chunking": {
                "dev": True,
                "staging": True,
                "canary": 0.05,  # 5%ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
                "prod": False
            },
            "response_caching": {
                "dev": True,
                "staging": True,
                "canary": 1.0,
                "prod": True
            }
        }
    
    def is_enabled(self, feature: str, stage: DeploymentStage, user_id: str = None) -> bool:
        """æ©Ÿèƒ½ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        if feature not in self.flags:
            return False
        
        flag_value = self.flags[feature].get(stage.value, False)
        
        # ãƒ–ãƒ¼ãƒ«å€¤ã®å ´åˆ
        if isinstance(flag_value, bool):
            return flag_value
        
        # å‰²åˆã®å ´åˆï¼ˆã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆï¼‰
        if isinstance(flag_value, float):
            if user_id:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒ™ãƒ¼ã‚¹ã®ä¸€è²«ã—ãŸåˆ¤å®š
                user_hash = hash(user_id) % 100
                return user_hash < (flag_value * 100)
            else:
                # ãƒ©ãƒ³ãƒ€ãƒ åˆ¤å®š
                return random.random() < flag_value
        
        return False

class HealthChecker:
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç®¡ç†"""
    
    def __init__(self):
        self.checks = {}
    
    def register_check(self, name: str, check_func: Callable) -> None:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–¢æ•°ã‚’ç™»éŒ²"""
        self.checks[name] = check_func
    
    def run_checks(self) -> Dict[str, Any]:
        """å…¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
        results = {
            "status": "healthy",
            "checks": {},
            "timestamp": datetime.now().isoformat()
        }
        
        for name, check_func in self.checks.items():
            try:
                start_time = time.time()
                result = check_func()
                duration = time.time() - start_time
                
                results["checks"][name] = {
                    "status": "pass" if result else "fail",
                    "duration_ms": round(duration * 1000, 2),
                    "details": result if isinstance(result, dict) else {}
                }
                
                if not result:
                    results["status"] = "unhealthy"
                    
            except Exception as e:
                results["checks"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                results["status"] = "unhealthy"
        
        return results

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–¢æ•°ã®ä¾‹
def check_vector_store() -> bool:
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ¥ç¶šãƒã‚§ãƒƒã‚¯"""
    try:
        # å®Ÿéš›ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ¥ç¶šãƒ†ã‚¹ãƒˆ
        return True
    except:
        return False

def check_llm_service() -> bool:
    """LLMã‚µãƒ¼ãƒ“ã‚¹ã®æ¥ç¶šãƒã‚§ãƒƒã‚¯"""
    try:
        # å®Ÿéš›ã®LLMã‚µãƒ¼ãƒ“ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ
        return True
    except:
        return False

# ä½¿ç”¨ä¾‹
def setup_health_checks() -> HealthChecker:
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    health_checker = HealthChecker()
    health_checker.register_check("vector_store", check_vector_store)
    health_checker.register_check("llm_service", check_llm_service)
    
    return health_checker
```

### ç›£è¦–ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### ä¸»è¦KPIã®å®šç¾©ã¨æ¸¬å®š

```python
from dataclasses import dataclass
from typing import Dict, List
import time
from datetime import datetime, timedelta

@dataclass
class RAGMetrics:
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    query_latency: float = 0.0          # ã‚¯ã‚¨ãƒªå¿œç­”æ™‚é–“ï¼ˆç§’ï¼‰
    retrieval_latency: float = 0.0      # æ¤œç´¢æ™‚é–“ï¼ˆç§’ï¼‰
    generation_latency: float = 0.0     # ç”Ÿæˆæ™‚é–“ï¼ˆç§’ï¼‰
    
    # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
    retrieval_precision: float = 0.0    # æ¤œç´¢ç²¾åº¦
    answer_relevance: float = 0.0       # å›ç­”é–¢é€£æ€§
    answer_completeness: float = 0.0    # å›ç­”å®Œå…¨æ€§
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    cpu_usage: float = 0.0              # CPUä½¿ç”¨ç‡
    memory_usage: float = 0.0           # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
    cache_hit_rate: float = 0.0         # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡
    
    # ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    user_satisfaction: float = 0.0      # ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦
    query_success_rate: float = 0.0     # ã‚¯ã‚¨ãƒªæˆåŠŸç‡
    daily_active_users: int = 0         # æ—¥æ¬¡ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼

class MetricsCollector:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.metrics_history = []
        self.current_metrics = RAGMetrics()
    
    def start_timer(self) -> float:
        """ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹"""
        return time.time()
    
    def end_timer(self, start_time: float) -> float:
        """ã‚¿ã‚¤ãƒãƒ¼çµ‚äº†ãƒ»çµŒéæ™‚é–“ã‚’è¿”ã™"""
        return time.time() - start_time
    
    def record_query_metrics(self, 
                           retrieval_time: float,
                           generation_time: float,
                           doc_count: int,
                           query_success: bool):
        """ã‚¯ã‚¨ãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        total_time = retrieval_time + generation_time
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "query_latency": total_time,
            "retrieval_latency": retrieval_time,
            "generation_latency": generation_time,
            "retrieved_doc_count": doc_count,
            "query_success": query_success
        }
        
        self.metrics_history.append(metrics)
    
    def get_daily_summary(self, date: datetime = None) -> Dict:
        """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if date is None:
            date = datetime.now()
        
        start_of_day = date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_of_day = start_of_day + timedelta(days=1)
        
        daily_metrics = [
            m for m in self.metrics_history
            if start_of_day <= datetime.fromisoformat(m["timestamp"]) < end_of_day
        ]
        
        if not daily_metrics:
            return {"date": date.date().isoformat(), "no_data": True}
        
        return {
            "date": date.date().isoformat(),
            "total_queries": len(daily_metrics),
            "avg_latency": sum(m["query_latency"] for m in daily_metrics) / len(daily_metrics),
            "success_rate": sum(1 for m in daily_metrics if m["query_success"]) / len(daily_metrics),
            "avg_retrieval_time": sum(m["retrieval_latency"] for m in daily_metrics) / len(daily_metrics),
            "avg_generation_time": sum(m["generation_latency"] for m in daily_metrics) / len(daily_metrics),
            "min_latency": min(m["query_latency"] for m in daily_metrics),
            "max_latency": max(m["query_latency"] for m in daily_metrics),
            "p95_latency": self._calculate_percentile(
                [m["query_latency"] for m in daily_metrics], 95
            )
        }
    
    def _calculate_percentile(self, values: List[float], percentile: int) -> float:
        """ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã‚’è¨ˆç®—"""
        sorted_values = sorted(values)
        index = int(len(sorted_values) * percentile / 100)
        return sorted_values[min(index, len(sorted_values) - 1)]
```

### ç¶™ç¶šçš„æ”¹å–„ã®ãŸã‚ã®å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ

> **ğŸ“ˆ ç¶™ç¶šçš„æ”¹å–„ã®é‡è¦æ€§**  
> RAGã‚·ã‚¹ãƒ†ãƒ ã¯ã€Œä½œã£ã¦çµ‚ã‚ã‚Šã€ã§ã¯ãªãã€ç¶™ç¶šçš„ãªæ”¹å–„ãŒå“è³ªå‘ä¸Šã®éµã¨ãªã‚Šã¾ã™ã€‚å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’æ´»ç”¨ã—ã¦ã€æ®µéšçš„ã«ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¦ã„ãã“ã¨ãŒé‡è¦ã§ã™ã€‚

#### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹

**ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã®é‡è¦æ€§**

> **ğŸ”§ å®Ÿè·µã‹ã‚‰å­¦ã‚“ã æ•™è¨“**  
> æ–‡æ›¸ã®æ€§è³ªã‚’ç†è§£ã›ãšã«ä¸€å¾‹çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’é©ç”¨ã™ã‚‹ã¨ã€æ¤œç´¢ç²¾åº¦ãŒå¤§ããä½ä¸‹ã—ã¾ã™ã€‚æŠ€è¡“æ–‡æ›¸ã€FAQã€å¯¾è©±ãƒ­ã‚°ãªã©ã€ãã‚Œãã‚Œã®ç‰¹æ€§ã«åˆã‚ã›ãŸæœ€é©åŒ–ãŒå¿…è¦ã§ã™ã€‚

- å˜ç´”ãªæ–‡å­—æ•°åˆ†å‰²ã‚ˆã‚Šã‚‚ã€æ„å‘³çš„ãªå¢ƒç•Œï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€æ®µè½ï¼‰ã‚’è€ƒæ…®ã—ãŸåˆ†å‰²ãŒåŠ¹æœçš„
- æ—¥æœ¬èªæ–‡æ›¸ã§ã¯ã€å¥èª­ç‚¹ã‚„æ”¹è¡Œã‚’é©åˆ‡ã«æ´»ç”¨ã™ã‚‹ã“ã¨ã§æ¤œç´¢ç²¾åº¦ãŒå‘ä¸Š
- ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯æ–‡æ›¸ã®æ€§è³ªã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦ï¼ˆæŠ€è¡“æ–‡æ›¸: 800æ–‡å­—ã€FAQ: 400æ–‡å­—ï¼‰

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®åŠ¹æœ**

> **âš–ï¸ ãƒãƒ©ãƒ³ã‚¹ã®é‡è¦æ€§**  
> å˜ä¸€ã®æ¤œç´¢æ‰‹æ³•ã§ã¯æ‰ãˆãã‚Œãªã„å¤šæ§˜ãªæ¤œç´¢ãƒ‹ãƒ¼ã‚ºã«å¯¾å¿œã™ã‚‹ãŸã‚ã€è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§æ¤œç´¢ã®ã€Œæ­»è§’ã€ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆæ„å‘³çš„é¡ä¼¼æ€§ï¼‰ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆèªå½™çš„ä¸€è‡´ï¼‰ã®çµ„ã¿åˆã‚ã›
- ç‰¹ã«ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹é–¢é€£ã®ã‚¯ã‚¨ãƒªã§ã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ãŒé‡è¦
- é‡ã¿ä»˜ã‘ï¼ˆãƒ™ã‚¯ãƒˆãƒ«70%ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰30%ï¼‰ã«ã‚ˆã‚Šç²¾åº¦ãŒå‘ä¸Š

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**

> **ğŸ“ æ§‹é€ åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ä¾¡å€¤**  
> LLMã«å¯¾ã™ã‚‹æŒ‡ç¤ºã‚’ä½“ç³»çš„ã«æ§‹é€ åŒ–ã™ã‚‹ã“ã¨ã§ã€å‡ºåŠ›ã®ä¸€è²«æ€§ã¨å“è³ªã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å„è¦ç´ ãŒæœãŸã™å½¹å‰²ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

- ã‚·ã‚¹ãƒ†ãƒ ãƒ­ãƒ¼ãƒ«ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ã‚¯ã‚¨ãƒªã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡ç¤ºã®æ®µéšçš„ãªæ§‹ç¯‰
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ¶é™ã‚’è€ƒæ…®ã—ãŸå‹•çš„ãªæ–‡æ›¸é¸æŠ
- ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æŒ‡å®š

**ãƒ­ã‚°ã¨ãƒ‡ãƒãƒƒã‚°ã®é‡è¦æ€§**

> **ğŸ” è¦‹ãˆã‚‹åŒ–ã«ã‚ˆã‚‹æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«**  
> ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œã‚’è©³ç´°ã«è¨˜éŒ²ãƒ»åˆ†æã™ã‚‹ã“ã¨ã§ã€å•é¡Œã®æ—©æœŸç™ºè¦‹ã¨ç¶™ç¶šçš„ãªæ”¹å–„ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ç‰¹ã«RAGã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ã©ã®æ–‡æ›¸ãŒé¸ã°ã‚Œã€ã©ã®ã‚ˆã†ãªå›ç­”ãŒç”Ÿæˆã•ã‚ŒãŸã‹ã®è¿½è·¡ãŒé‡è¦ã§ã™ã€‚

- æ§‹é€ åŒ–ãƒ­ã‚°ï¼ˆJSONå½¢å¼ï¼‰ã«ã‚ˆã‚Šåˆ†æãŒå®¹æ˜“
- å®Ÿéš›ã®LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€å–å¾—æ–‡æ›¸ã€å¿œç­”æ™‚é–“ã®è¨˜éŒ²
- ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®è©³ç´°ãªæƒ…å ±åé›†

#### ä»Šå¾Œã®æ”¹å–„æ–¹å‘

**æ¤œç´¢ç²¾åº¦ã®å‘ä¸Š**

> **ğŸ¯ æ¬¡ä¸–ä»£æ¤œç´¢æŠ€è¡“ã®æ–¹å‘æ€§**  
> é™çš„ãªæ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å­¦ç¿’ã™ã‚‹å‹•çš„ãªã‚·ã‚¹ãƒ†ãƒ ã¸ã®é€²åŒ–ãŒé‡è¦ã§ã™ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®åŒç¾©èªè¾æ›¸ã€ã‚¯ã‚¨ãƒªã®æ„å›³ç†è§£ã€æ–‡è„ˆã«å¿œã˜ãŸçµæœãƒ©ãƒ³ã‚­ãƒ³ã‚°ãªã©ã®æŠ€è¡“ãŒéµã¨ãªã‚Šã¾ã™ã€‚

```python
# ã‚ˆã‚Šé«˜åº¦ãªã‚¯ã‚¨ãƒªæ‹¡å¼µ
def advanced_query_expansion(query: str) -> List[str]:
    """åŒç¾©èªã€é–¢é€£èªã‚’ç”¨ã„ãŸã‚¯ã‚¨ãƒªæ‹¡å¼µ"""
    expanded_queries = [query]
    
    # åŒç¾©èªè¾æ›¸ã®æ´»ç”¨
    synonyms = load_domain_synonyms()
    for word, syns in synonyms.items():
        if word in query:
            for syn in syns:
                expanded_queries.append(query.replace(word, syn))
    
    return expanded_queries

# ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å®Ÿè£…
def rerank_results(query: str, initial_results: List) -> List:
    """ã‚ˆã‚Šç²¾å¯†ãªãƒ¢ãƒ‡ãƒ«ã§æ¤œç´¢çµæœã‚’å†è©•ä¾¡"""
    # Cross-encoderã‚„ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    pass
```

**ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ**

> **ğŸ–¼ï¸ æƒ…å ±ã®å¤šæ§˜æ€§ã¸ã®å¯¾å¿œ**  
> ç¾ä»£ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«ã¯æ–‡å­—æƒ…å ±ã ã‘ã§ãªãã€å›³è¡¨ã€ç”»åƒã€å‹•ç”»ãªã©ã‚‚å«ã¾ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’çµ±åˆçš„ã«æ¤œç´¢ãƒ»æ´»ç”¨ã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«ã‚ˆã‚Šã€ã‚ˆã‚Šè±Šå¯Œã§æ­£ç¢ºãªå›ç­”ã®æä¾›ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```python
# ç”»åƒãƒ»å›³è¡¨ã‚’å«ã‚€æ–‡æ›¸ã®å‡¦ç†
def process_multimodal_document(doc_path: str) -> Dict:
    """ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆã—ãŸæ–‡æ›¸å‡¦ç†"""
    return {
        "text_chunks": extract_text_chunks(doc_path),
        "image_descriptions": extract_and_describe_images(doc_path),
        "table_data": extract_structured_tables(doc_path)
    }
```

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’**

> **ğŸ”„ è‡ªå·±æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç¾**  
> ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ç¶™ç¶šçš„ã«å­¦ç¿’ã—ã€ã‚·ã‚¹ãƒ†ãƒ è‡ªä½“ãŒæˆé•·ã—ã¦ã„ãä»•çµ„ã¿ã®æ§‹ç¯‰ãŒé‡è¦ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä½¿ãˆã°ä½¿ã†ã»ã©ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

```python
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®å­¦ç¿’
class FeedbackLearning:
    def collect_feedback(self, query: str, response: str, rating: int):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åé›†"""
        pass
    
    def update_retrieval_weights(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãæ¤œç´¢é‡ã¿ã‚’èª¿æ•´"""
        pass
```

#### é–‹ç™ºãƒãƒ¼ãƒ å‘ã‘ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

**ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**

> **ğŸ‘¥ å“è³ªä¿è¨¼ã®é‡è¦æ€§**  
> RAGã‚·ã‚¹ãƒ†ãƒ ã¯è¤‡æ•°ã®æŠ€è¡“è¦ç´ ãŒé€£æºã™ã‚‹è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å“è³ªã‚’ç¢ºä¿ã—ã€çµ±åˆæ™‚ã®å•é¡Œã‚’æœªç„¶ã«é˜²ããŸã‚ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒé‡è¦ã§ã™ã€‚

- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé©åˆ‡ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] ãƒ­ã‚°ãŒååˆ†ãªæƒ…å ±ã‚’å«ã‚“ã§ã„ã‚‹ã‹
- [ ] æ©Ÿå¯†æƒ…å ±ãŒãƒ­ã‚°ã«å‡ºåŠ›ã•ã‚Œã¦ã„ãªã„ã‹
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è€ƒæ…®ãŒã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã‹

**é–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**

> **ğŸ”§ é–‹ç™ºåŠ¹ç‡åŒ–ã®åŸºç›¤**  
> é–‹ç™ºãƒãƒ¼ãƒ å…¨ä½“ã§ä¸€è²«ã—ãŸç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã€ç’°å¢ƒå·®ç•°ã«ã‚ˆã‚‹å•é¡Œã‚’é˜²ãã€åŠ¹ç‡çš„ãªé–‹ç™ºã‚’å®Ÿç¾ã§ãã¾ã™ã€‚ç‰¹ã«RAGã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€LLM APIã€ãƒ™ã‚¯ãƒˆãƒ«DBã€Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãªã©è¤‡æ•°ã®ã‚µãƒ¼ãƒ“ã‚¹ã®é€£æºãŒå¿…è¦ã§ã™ã€‚

```bash
# é–‹ç™ºç”¨ã®è¨­å®š
cp .env.example .env.dev
# DEBUG_MODE=true ã‚’è¨­å®š

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
python scripts/create_test_data.py

# å“è³ªãƒã‚§ãƒƒã‚¯
pre-commit install
black src/
flake8 src/
pytest tests/
```

**æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤å‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**

> **ğŸš€ æœ¬ç•ªé‹ç”¨ã¸ã®æº–å‚™**  
> RAGã‚·ã‚¹ãƒ†ãƒ ã‚’æœ¬ç•ªç’°å¢ƒã§å®‰å®šé‹ç”¨ã™ã‚‹ãŸã‚ã«ã¯ã€é–‹ç™ºç’°å¢ƒã§ã®å‹•ä½œç¢ºèªã ã‘ã§ã¯ä¸ååˆ†ã§ã™ã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ç›£è¦–ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãªã©ã€æœ¬ç•ªç‰¹æœ‰ã®è¦ä»¶ã«å¯¾ã™ã‚‹ååˆ†ãªæº–å‚™ãŒå¿…è¦ã§ã™ã€‚

- [ ] å…¨ãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹ã—ã¦ã„ã‚‹ã‹
- [ ] æ©Ÿå¯†æƒ…å ±ãŒç’°å¢ƒå¤‰æ•°ã§ç®¡ç†ã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå‹•ä½œã™ã‚‹ã‹
- [ ] ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †ãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ã‹

---

## ã¾ã¨ã‚

ã“ã®RAGãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚¬ã‚¤ãƒ‰ã§ã¯ã€å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§RAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ãƒ»é‹ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªçŸ¥è­˜ã¨æ‰‹æ³•ã‚’åŒ…æ‹¬çš„ã«è§£èª¬ã—ã¾ã—ãŸã€‚æŠ€è¡“çš„ãªå®Ÿè£…ã ã‘ã§ãªãã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã€ç¶™ç¶šçš„æ”¹å–„ã¾ã§å«ã‚ã‚‹ã“ã¨ã§ã€å …ç‰¢ã§æ‹¡å¼µæ€§ã®é«˜ã„RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«å½¹ç«‹ã¤ãƒªã‚½ãƒ¼ã‚¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚

RAGæŠ€è¡“ã¯æ€¥é€Ÿã«é€²æ­©ã—ã¦ã„ã‚‹åˆ†é‡ã§ã™ãŒã€ã“ã“ã§ç¤ºã—ãŸåŸºæœ¬çš„ãªåŸå‰‡ã¨å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ä»Šå¾Œã®ç™ºå±•ã«ã‚‚å¯¾å¿œã§ãã‚‹å …å›ºãªåŸºç›¤ã¨ãªã‚‹ã¯ãšã§ã™ã€‚
